{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python Boot Camp\n",
    "\n",
    "***\n",
    "\n",
    "## Table of contents:\n",
    "\n",
    "* [Images as arrays](#first)   \n",
    "* [Image color space](#second)\n",
    "* [Image data types](#third)\n",
    "* [Plotting images](#fourth)\n",
    "* [Images in Deep Learning frameworks](#fifth)\n",
    "* [Simple image manipulation](#sixth)\n",
    "* [Loading a set of images](#seventh)\n",
    "* [Segmentation maps](#eighth)\n",
    "* [Batching](#ninth)\n",
    "* [Data augmentation](#tenth)\n",
    "* [Using Tensorboard](#eleventh)\n",
    "* [Convolutions](#twelveth)\n",
    "\n",
    "\n",
    "*** \n",
    "\n",
    "Welcome!\n",
    "In this notebook, we will go through some basic image processing in Python and familiarize ourselves with different utilities that can be useful for any Deep Learning pipeline, utilities provides through libraries like `skimage`, `imgaug`, `glob`, `tqdm` and more.\n",
    "\n",
    "We will be using sample images from this [three-dimensional X-ray microtomography thalamocortical dataset](https://github.com/nerdslab/xray-thc), used to characterize brain heterogeneity. These samples, imaged in the Striatum and Hypothalamus regions of a mouse brain, were annotated to get microstructure segmentation maps (of cell bodies, blood vessels, and myelinated axons). The full dataset is available on [bossdb](https://bossdb.org/project/prasad2020)! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as pl \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Images as arrays <a class=\"anchor\" id=\"first\"></a>\n",
    "\n",
    "Images are represented as numpy arrays of shape (height, width, channels).\n",
    "\n",
    "![RGB image as a numpy array](./assets/image_as_array.png)\n",
    "\n",
    "<div style=\"text-align: right\"> Credit: <a href=\"https://e2eml.school/convert_rgb_to_grayscale.html\">Brandon Rohrerâ€™s Blog</a></div>\n",
    "\n",
    "\n",
    "Multiple utilities/packages exist to read images from files in Python,\n",
    "we will use `skimage.io.imread`.\n",
    "\n",
    "\n",
    "If you look in the directory containing this notebook, you will find a folder called data which includes some tiff files. \n",
    "Here our images have the .tiff extenstion and all start with \"img\".\n",
    "\n",
    "Let's load one image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type: <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "from skimage import io\n",
    "\n",
    "img = io.imread('data/img_650_1162__600_1112__0.tiff')\n",
    "print('Type:', type(img))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image color space <a class=\"anchor\" id=\"second\"></a>\n",
    "If the image is in grayscale, then the number of channels is equal to 1,\n",
    "in which case the array can also be of shape (height, width).\n",
    "If the image is RGB, then the number of channels is 3\n",
    "with each channel encoding the red, green and blue components.\n",
    "\n",
    "**Q: Is `img` RGB or Grayscale?**\n",
    "\n",
    "<div style=\"text-align: right\"> Help: <a href=\"https://s3.amazonaws.com/assets.datacamp.com/blog_assets/Numpy_Python_Cheat_Sheet.pdf\">Numpy cheatsheet</a></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(512, 512)\n"
     ]
    }
   ],
   "source": [
    "# Solution (it's grayscale, has no channel dimension)\n",
    "\n",
    "print(img.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q: Reshape the image such that its shape is (height, width, 1)**\n",
    "\n",
    "<div style=\"text-align: right\"> Hint: <a href=\"https://numpy.org/doc/stable/reference/generated/numpy.expand_dims.html\">expand dims</a></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(512, 512, 1)\n"
     ]
    }
   ],
   "source": [
    "# Solution\n",
    "\n",
    "img = np.expand_dims(img, axis=2)\n",
    "print(img.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image data types <a class=\"anchor\" id=\"third\"></a>\n",
    "\n",
    "\n",
    "Images can be represented by a variety of data types. The following is a list of the most common types:\n",
    "- `bool`: binary, 0 or 1\n",
    "- `uint8`: unsigned integers, 0 to 255 range\n",
    "- `float`: -1 to 1 or 0 to 1\n",
    "\n",
    "**Q: What is the data type of `img`? What are the minimum and maximum intensity values?**\n",
    "<div style=\"text-align: right\"> Help: <a href=\"https://s3.amazonaws.com/assets.datacamp.com/blog_assets/Numpy_Python_Cheat_Sheet.pdf\">Numpy cheatsheet</a></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'ellipsis' object has no attribute 'dtype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Solution\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mimg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(img\u001b[38;5;241m.\u001b[39mmin(), img\u001b[38;5;241m.\u001b[39mmax())\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'ellipsis' object has no attribute 'dtype'"
     ]
    }
   ],
   "source": [
    "# Solution\n",
    "\n",
    "print(img.dtype)\n",
    "print(img.min(), img.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting images <a class=\"anchor\" id=\"fourth\"></a>\n",
    "\n",
    "Using `matplotlib` we can visualize the image. When the image is in grayscale,\n",
    "a colormap can be specified using the `cmap` argument. By default,\n",
    "the colormap is `viridis`, we will use `gray`.\n",
    "\n",
    "> Useful `matplotlib` ressources:\n",
    "> - [matplotlib cheatsheets](https://github.com/matplotlib/cheatsheets)\n",
    "> - [colormaps in matplotlib](https://matplotlib.org/stable/tutorials/colors/colormaps.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "pl.imshow(img, cmap='gray')\n",
    "pl.axis('off')\n",
    "pl.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Images in Deep Learning frameworks <a class=\"anchor\" id=\"fifth\"></a>\n",
    "\n",
    "\n",
    "In pytorch, tensorflow or jax, (ML libraries that we will soon be using) images are represented as (channels, height, width)\n",
    "and are rescaled to be in the [0, 1] range.\n",
    "\n",
    "**Q: Generate a new image that respects these conventions.\n",
    "You can use [`np.transpose`](https://numpy.org/doc/stable/reference/generated/numpy.transpose.html).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_tensor = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution\n",
    "\n",
    "img_tensor = np.transpose(img, (2, 0, 1))\n",
    "print(img_tensor.shape)\n",
    "\n",
    "img_tensor = img_tensor / 255.\n",
    "print(img_tensor.dtype)\n",
    "print(img_tensor.min(), img_tensor.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple image manipulation <a class=\"anchor\" id=\"sixth\"></a>\n",
    "\n",
    "Given that images are numpy arrays, we can take advantage of the powerful\n",
    " [indexing](https://numpy.org/doc/stable/reference/arrays.indexing.html)\n",
    " to perform simple transformations like cropping, downsampling and flipping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# cropping\n",
    "cropped_img = img[10:310, 50:350]\n",
    "\n",
    "# downsampling\n",
    "factor = 2\n",
    "downsampled_img = img[::factor, ::factor]\n",
    "\n",
    "# horizontal flip\n",
    "hflip_img = img[:,::-1]\n",
    "\n",
    "fig, axs = pl.subplots(1, 4, figsize=(15,5), frameon=False)\n",
    "\n",
    "axs[0].imshow(img, cmap='gray')\n",
    "axs[0].set_title('Original image\\nSize = {}'.format(str(img.shape)))\n",
    "axs[0].axis('off')\n",
    "\n",
    "axs[1].imshow(cropped_img, cmap='gray')\n",
    "axs[1].set_title('Cropped image\\nSize = {}'.format(str(cropped_img.shape)))\n",
    "axs[1].axis('off')\n",
    "\n",
    "axs[2].imshow(downsampled_img, cmap='gray')\n",
    "axs[2].set_title('Downsampled image\\nSize = {}'.format(str(downsampled_img.shape)))\n",
    "axs[2].axis('off')\n",
    "\n",
    "axs[3].imshow(hflip_img, cmap='gray')\n",
    "axs[3].set_title('Horizontal Flip\\n Size = {}'.format(str(hflip_img.shape)))\n",
    "axs[3].axis('off')\n",
    "pl.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q: Apply the following transformations:**\n",
    "- Center crop of size (256, 256)\n",
    "- Vertical flip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution\n",
    "\n",
    "# center crop\n",
    "height, width, _ = img.shape\n",
    "top, bottom = height // 4, 3 * height // 4\n",
    "left, right =  width // 4, 3 * width // 4\n",
    "\n",
    "cropped_img = img[top:bottom, left:right]\n",
    "\n",
    "# vertical flip\n",
    "vflip_img = img[::-1]\n",
    "\n",
    "fig, axs = pl.subplots(1, 3, figsize=(15,5))\n",
    "\n",
    "axs[0].imshow(img, cmap='gray')\n",
    "axs[0].set_title('Original image\\nSize = {}'.format(str(img.shape)))\n",
    "axs[0].axis('off')\n",
    "\n",
    "axs[1].imshow(cropped_img, cmap='gray')\n",
    "axs[1].set_title('Center-cropped image\\nSize = {}'.format(str(cropped_img.shape)))\n",
    "axs[1].axis('off')\n",
    "\n",
    "axs[2].imshow(vflip_img, cmap='gray')\n",
    "axs[2].set_title('Vertical Flip\\nSize = {}'.format(str(vflip_img.shape)))\n",
    "axs[2].axis('off')\n",
    "\n",
    "pl.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\"><h1>Checkpoint 1</h1>\n",
    " \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading a set of images <a class=\"anchor\" id=\"seventh\"></a>\n",
    "\n",
    "Given a set of images in a folder, we need to be able to easily find the pathnames and load them in. \n",
    "`glob` is a standard package that provides a utility for finding all pathnames that match a given pattern.\n",
    "\n",
    "Here our images have the `.tiff` extenstion and all start with `img`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "\n",
    "root = 'data/'\n",
    "img_filenames = glob(os.path.join(root, 'img*.tiff'))\n",
    "\n",
    "print('Found:')\n",
    "for img_filename in img_filenames:\n",
    "    print(' ', img_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now load the images.\n",
    "\n",
    "We will use `tqdm` to track progress (even though we only have a small number of images here). `tqdm` provides a progress bar that simply wraps around any iterable, making it useful for tracking training progress for example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "imgs = []\n",
    "for fname in tqdm(img_filenames):\n",
    "    img = io.imread(fname)\n",
    "    img = np.expand_dims(img, axis=2)\n",
    "    imgs.append(img)\n",
    "\n",
    "fig, axs = pl.subplots(1, 4, figsize=(15,5))\n",
    "for i in range(4):\n",
    "    axs[i].imshow(imgs[i], cmap='gray')\n",
    "    axs[i].axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Segmentation maps <a class=\"anchor\" id=\"eighth\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We often want to \"segment\" data - i.e: assign labels to parts of images. \n",
    "\n",
    "This dataset has already been segmented for us into four categories: cell bodies, blood vessels, axons, as well as background. \n",
    "\n",
    "For each image file, you will find the corresponding segmentation file in the data folder. These begin with \"annos\" and have the .tiff extension.\n",
    "\n",
    "**Q: Using what we have seen thus far, load the segmentation maps into a list called `seg_maps` and visualize them. Be careful with the file ordering.**\n",
    "<div style=\"text-align: right\"> <a href=\"https://pythonbasics.org/replace/\">Hint</a></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seg_filenames = ...\n",
    "\n",
    "seg_maps = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution\n",
    "\n",
    "annos_filenames = [filename.replace('img', 'annos') for filename in img_filenames]\n",
    "\n",
    "seg_maps = []\n",
    "for fname in annos_filenames:\n",
    "    seg_map = io.imread(fname)\n",
    "    seg_maps.append(seg_map)\n",
    "\n",
    "fig, axs = pl.subplots(1, 4, figsize=(15,5))\n",
    "for i in range(4):\n",
    "    axs[i].imshow(seg_maps[i])\n",
    "    axs[i].axis('off')\n",
    "\n",
    "pl.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will notice that there are 4 unique values in the segmentation maps, each corresponding to one of these four classes:\n",
    "- 0: background\n",
    "- 1: cell\n",
    "- 2: blood vessel\n",
    "- 3: axon\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q: What is the data type of these segmentation maps?**\n",
    "\n",
    "**What data types do you know of? How should one choose what data type to use? Should segmentation maps necessarily be saved in the same data type as their respective images? If yes, why? In no, why not?** \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution\n",
    "\n",
    "seg_maps[0].dtype\n",
    "\n",
    "# Yes, raw images could be stored as integers or floats. \n",
    "# Segmentation maps will generally be stored as integers since each label will correspond to a specific value\n",
    "# Segmentation maps can be stored with different sizes depending on how many labels are in the data\n",
    "# eg small datasets with less than 255 labels can be stored as uint8 while medium - large datasets could be\n",
    "# stored as uint16, uint32, or uint64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Segmentation Maps\n",
    "\n",
    "Let's visualize the segmentation map of one image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seg_map_example = seg_maps[0]\n",
    "\n",
    "labels = ['background', 'cells', 'blood vessels', 'axons']\n",
    "\n",
    "fig, axs = pl.subplots(1, 4, figsize=(15,5))\n",
    "for i in range(4):\n",
    "    tmp = np.zeros_like(seg_map_example)\n",
    "    tmp[seg_map==i] = 1\n",
    "    axs[i].imshow(tmp, cmap='gray')\n",
    "    axs[i].set_title(labels[i])\n",
    "    axs[i].axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batching <a class=\"anchor\" id=\"ninth\"></a>\n",
    "\n",
    "In ML/DL, we often have to deal with very large datasets. It soon becomes inefficient to process all the data at once so it's useful to split the data into \"batches\" that we can process individually. So for purely reasons of computational cost this is often useful.\n",
    "\n",
    "We will also see another reason for which batching can be useful - for instance when running gradient descent on none convex landscapes. Here computing the gradient on a subset of the data gives us an appoximate/noisy gradient making it less likely for us to end up stuck in local minimum. This is what we call \"stochastic gradient descent\".\n",
    "\n",
    "Let us make our first batch of images, containing $B$ images. \n",
    "The shape of the batch will thus get an additional \"batch dimension\" at the first dimension: (batch_size, channels, height, width).\n",
    "\n",
    "**Q: Make a batch out of the four images (this will be a 4D numpy array)**\n",
    "\n",
    "\n",
    "<div style=\"text-align: right\"> <a href=\"https://numpy.org/doc/stable/reference/generated/numpy.stack.html\">Hint</a></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution\n",
    "\n",
    "img_tensors = [img.transpose((2, 0, 1)) for img in imgs]\n",
    "batch = np.stack(img_tensors)\n",
    "\n",
    "print(batch.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\"><h1>Checkpoint 2</h1>\n",
    " \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutions <a class=\"anchor\" id=\"twelveth\"></a>\n",
    "\n",
    "Convolutions are the elementary operations used in CNNs. The image (and later, the feature maps) are convolved with multiple kernels which weights are learned. Below is a visual of the pixel values in the output matrix (green) being computed from neighboring pixels in the input matrix (blue). \n",
    "\n",
    "![](https://raw.githubusercontent.com/vdumoulin/conv_arithmetic/master/gif/no_padding_no_strides.gif)\n",
    "\n",
    "<div style=\"text-align: right\"> Credit: <a href=\"https://github.com/vdumoulin/conv_arithmetic\">Vincent Dumoulin, Francesco Visin</a></div>\n",
    "\n",
    "**Q: Implement a function that performs \"convolution\". Assume that your image is square and that your kernel is square and has an odd width. Note that your output image will be smaller (we won't use padding for now).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv2d(img, kernel):\n",
    "    m = img.shape[0]  #size of original image\n",
    "    k = kernel.shape[0]  #size of filter\n",
    "    \n",
    "    mc = ... #size of the new image after convolution\n",
    "    # NB: there's no one right answer\n",
    "    \n",
    "    conv_img = np.zeros((mc,mc))\n",
    "    \n",
    "    # perform convolution\n",
    "    for ii in range(mc):\n",
    "        for jj in range(mc):\n",
    "            conv_img[ii,jj] = ...\n",
    "            # see Florian's slide for hing\n",
    "            \n",
    "    \n",
    "    return conv_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution\n",
    "\n",
    "def conv2d(img, kernel):\n",
    "    m = img.shape[0]  #size of original image\n",
    "    k = kernel.shape[0]  #size of filter\n",
    "    \n",
    "    mc = m - k + 1 #size of the new image after convolution\n",
    "    conv_img = np.zeros((mc,mc))\n",
    "    \n",
    "    for ii in range(mc):\n",
    "        for jj in range(mc):\n",
    "            conv_img[ii,jj] = np.sum(img[ii:ii+k, jj:jj+k] * kernel)\n",
    "    return conv_img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Smoothing filter\n",
    "Convolving the image with a smoothing filter is equivalent to replacing the value of each pixel with the average pixel value within a window of size $dxd$ around it.\n",
    "\n",
    "**Q: Design a smoothing filter. Try different values of d.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel = ... # try a kernel size of 10, then try some different values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = imgs[0][:,:,0] # choose one example image\n",
    "smoothed_img = ...\n",
    "# visualize your smoothed images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution\n",
    "kernel = np.ones((10, 10)) / 100\n",
    "smoothed_img = conv2d(img, kernel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare the original and smoothed image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = pl.subplots(ncols = 2)\n",
    "axs[0].imshow(img, cmap='gray')\n",
    "axs[0].set_title('original')\n",
    "axs[1].imshow(smoothed_img, cmap='gray')\n",
    "axs[1].set_title('smoothed image')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sobel filter\n",
    "The following is known as the Sobel filter:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "    1 & 2 & 1 \\\\\n",
    "    0 & 0 & 0 \\\\\n",
    "    -1 & -2 & -1 \n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "**Q: What pattern would produce the largest output when convolved with this filter? (assuming the sum of the pattern is capped)**\n",
    "\n",
    "**Q: Apply the Sobel filter and describe what it does**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel = ... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution\n",
    "kernel = np.array([[ 1,  2,  1], \n",
    "                   [ 0,  0,  0], \n",
    "                   [-1, -2, -1]])\n",
    "conv_img = conv2d(imgs[0], kernel)\n",
    "\n",
    "pl.figure(figsize=(8, 8))\n",
    "pl.imshow(conv_img, cmap='gray')\n",
    "pl.axis('off')\n",
    "pl.title('Sobel filter')\n",
    "pl.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\"><h1>Checkpoint 3</h1>\n",
    " \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Data augmentation <a class=\"anchor\" id=\"tenth\"></a>\n",
    "\n",
    "Having collected your hard earned data you want to make the most of it. In ML/DL, we're often limited by the size of our the training set. How could we artificially inflate our data to provide more input to our model and help it generalize better?\n",
    "\n",
    "One trick is to make simple transformations to our data such as rotating it or adding noise - this process is generally called \"data augementation\" (DA) and is widely used in ML.  \n",
    "\n",
    "`imgaug` is a Python library that provides a very extensive set of image augmentations,\n",
    "and that seamlessly handles complex annotations like segmentation maps, bounding boxes or keypoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import imgaug as ia\n",
    "from imgaug import augmenters as iaa\n",
    "\n",
    "# fix the random seed\n",
    "ia.seed(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Augmenting an image\n",
    "To use an augmentation, we can instantiate an `Augmenter` with a set of hyperparamters.\n",
    "With affine transforamtions for example, we can specify the range of the rotation angle to be `(-45, 45)`.\n",
    "\n",
    "In `imgaug`, the channel-axis is always expected to be the last axis and may be skipped for grayscale images, it is\n",
    "also recommended to work with the `uint8` dtype."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "rotate = iaa.Affine(rotate=(-45, 45))\n",
    "img_aug = rotate(image=imgs[0])\n",
    "\n",
    "pl.imshow(img_aug, cmap='gray')\n",
    "pl.axis('off')\n",
    "pl.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Note: Try running the previous cell again.\n",
    "\n",
    "#### Augmenting image AND segmentation map\n",
    "\n",
    "When applying certain augmentations, we want to make sure that the segmentation map is changed accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from imgaug.augmentables.segmaps import SegmentationMapsOnImage\n",
    "\n",
    "# convert array to SegmentationMapsOnImage instance\n",
    "seg_map = SegmentationMapsOnImage(seg_maps[0], shape=imgs[0].shape)\n",
    "# augment\n",
    "img_aug, seg_map_aug = rotate(image=imgs[0], segmentation_maps=seg_map)\n",
    "\n",
    "fig, axs = pl.subplots(1, 2, figsize=(8,5))\n",
    "axs[0].imshow(img_aug, cmap='gray')\n",
    "axs[0].axis('off')\n",
    "\n",
    "axs[1].imshow(seg_map_aug.draw()[0], cmap='gray')\n",
    "axs[1].axis('off')\n",
    "pl.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Applying multiple augmentations\n",
    "We can compose multiple augmentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "seq = iaa.Sequential([\n",
    "    iaa.AdditiveGaussianNoise(scale=(0, 30)),\n",
    "    iaa.pillike.FilterEdgeEnhanceMore(),\n",
    "    iaa.Crop(percent=(0., 0.2))\n",
    "])\n",
    "\n",
    "imgs_aug = seq(images=imgs)\n",
    "\n",
    "fig, axs = pl.subplots(2, 1, figsize=(20,10))\n",
    "axs[0].imshow(np.concatenate((imgs), axis=1), cmap='gray')\n",
    "axs[0].axis('off')\n",
    "\n",
    "axs[1].imshow(np.concatenate((imgs_aug), axis=1), cmap='gray')\n",
    "axs[1].axis('off')\n",
    "pl.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**Q: Famalirize yourself with the different augmentations available through `imgaug`. Refer to the [examples](https://github.com/aleju/imgaug) and the [documentation](https://imgaug.readthedocs.io/en/latest/). Identify and apply augmentations that you think are interesting.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Possible Solution\n",
    "seq = iaa.Sequential([\n",
    "    iaa.AdditiveGaussianNoise(scale=(0, 30)),\n",
    "    iaa.GaussianBlur(sigma=(0.0, 3.0)),\n",
    "    iaa.Sharpen(alpha=(0.0, 1.0), lightness=(0.75, 2.0)),\n",
    "    iaa.pillike.FilterEdgeEnhanceMore(),\n",
    "    iaa.Crop(percent=(0., 0.2))\n",
    "])\n",
    "\n",
    "imgs_aug = seq(images=imgs)\n",
    "\n",
    "fig, axs = pl.subplots(2, 1, figsize=(20,10))\n",
    "axs[0].imshow(np.concatenate((imgs), axis=1), cmap='gray')\n",
    "axs[0].axis('off')\n",
    "\n",
    "axs[1].imshow(np.concatenate((imgs_aug), axis=1), cmap='gray')\n",
    "axs[1].axis('off')\n",
    "pl.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
