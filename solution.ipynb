{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b356e976",
   "metadata": {},
   "source": [
    "# Python Boot Camp\n",
    "\n",
    "\n",
    "Welcome! 😃👋\n",
    "\n",
    "In this notebook, we will go through some basic image processing in Python, come across standard tasks required while setting up deep learning pipelines, and familiarize ourselves with popular packages such as `glob`, `tifffile`, `tqdm` and more.\n",
    "We will learn about:\n",
    "- Loading images (This is important as images are the primary input to most deep learning models)\n",
    "- Normalizing images (This is important as it helps in faster convergence of models becuse it helps in reducing the scale of the input data and hence the scale of the gradients)\n",
    "- Cropping images (This is important as it helps in creating smaller images from the original images which is useful for training models in a memory efficient way)\n",
    "- Downsampling images (This is important as it helps in reducing the size of the images which is useful for training models in a memory efficient way)\n",
    "- Flipping images (This is important as it helps in creating new images from the original data which is useful for training models in a memory efficient way)\n",
    "- Batching images (As we train in a SGD manner, batching is important as it helps in training the model in a memory efficient way and smoothens the optimization process)\n",
    "- Convolutions (This is important as it is the primary operation in Convolutional Neural Networks)\n",
    "- Data Augmentation (This is important as it helps in artificially increasing the size of the training data which is useful for training models in a data efficient way)\n",
    "\n",
    "\n",
    "We will be using sample images from the *MoNuSeg* dataset provided by [Kumar et al, 2018](https://ieeexplore.ieee.org/document/8880654). The data was publicly made available [here](https://monuseg.grand-challenge.org/) by the authors of the publication.\n",
    "\n",
    "This dataset shows Hematoxylin and Eosin (H&E) Stained Images showing nuclei in different shapes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f1984f0",
   "metadata": {},
   "source": [
    "## Chapter 0: Downloading data from an external url"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "147a2fae",
   "metadata": {},
   "source": [
    "Let us first download the images from an external url.\n",
    "To do so, we need to import some commonly used dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f0f5d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path # this library is used to handle file paths \n",
    "import urllib.request, zipfile # urllib is used to download files from the internet, zipfile is used to extract zip files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d9c68fd",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "Here, below is a helper function to download the data from an external url specified by argument `zip_url` and save it to a local directory specified by argument `project_name`. Let's execute the function (No output expected yet!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "215b951a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_data(zip_url, project_name):\n",
    "    zip_path = Path(project_name + \".zip\")\n",
    "    if zip_path.exists():\n",
    "        print(\"Zip file was downloaded and extracted before!\")\n",
    "    else:\n",
    "        urllib.request.urlretrieve(zip_url, zip_path)\n",
    "        print(\"Downloaded data as {}\".format(zip_path))\n",
    "    with zipfile.ZipFile(zip_path, \"r\") as zip_ref:\n",
    "        zip_ref.extractall(\"./\")\n",
    "    print(\"Unzipped data to {}\".format(Path(project_name)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fd09e8e",
   "metadata": {},
   "source": [
    "Now we call the function `extract_data` specifying desirable values of the arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cb4506a",
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_data(\n",
    "    zip_url=\"https://owncloud.mpi-cbg.de/index.php/s/xwYonC9LucjLsY6/download\",\n",
    "    project_name=\"monuseg-2018\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fff22461",
   "metadata": {},
   "source": [
    "### Task 0.1\n",
    "Click on the `Files` directory (left panel) and check if some images exist within the `monuseg-2018` directory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e21fbd48",
   "metadata": {},
   "source": [
    "### Task 0.2\n",
    "Can you use a bash command to programmatically count the number of images and masks present in the `download/images` directory ?\n",
    "\n",
    "*Hint*: Use `!ls -l <path> | wc - l` (you can run any bash command in a jupyter notebook by prefixing it with `!`) [Read more on this [here](https://linuxstans.com/wp-content/uploads/2023/06/bash-cheat-sheet.png)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4594e732",
   "metadata": {
    "tags": [
     "task"
    ]
   },
   "outputs": [],
   "source": [
    "##########################\n",
    "######## To Do ###########\n",
    "##########################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92fb5b73",
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "##########################\n",
    "####### Solution #########\n",
    "##########################\n",
    "\n",
    "!ls -l monuseg-2018/download/images | wc -l"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7130238c",
   "metadata": {},
   "source": [
    "## Chapter 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49226fba",
   "metadata": {},
   "source": [
    "### Images as arrays <a class=\"anchor\" name=\"first\"></a>\n",
    "\n",
    "2D Images are often represented as numpy arrays of shape (`height`, `width`, `num_channels`).\n",
    "\n",
    "![RGB image as a np array](https://github.com/dlmbl/boot/assets/34229641/ce1ad3f3-dc34-46d1-b301-198768fbc369)\n",
    "\n",
    "<div style=\"text-align: right\"> Credit: <a href=\"https://e2eml.school/convert_rgb_to_grayscale.html\">Brandon Rohrer’s Blog</a></div>\n",
    "\n",
    "\n",
    "Multiple utilities/packages exist to read images from files in Python.\n",
    "For example, one can use `tifffile.imread` to read `*.tif` images. <br>Another good package is `skimage.io.imread`.\n",
    "\n",
    "\n",
    "If you look in the directory (`monuseg-2018/download`), you can see directories called `images` and `masks`.\n",
    "\n",
    "Let's load one image and visualize it using `matplotlib.pyplot.imshow`. <br>\n",
    "`matplotlib.pyplot.imshow` is the standard way to show images in jupyter notebooks!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b186b65c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tifffile import imread\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "img = imread(\"monuseg-2018/download/images/TCGA-2Z-A9J9-01A-01-TS1.tif\")\n",
    "print(f\"Image `img` has type {type(img)}\")  # variable type\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f660e21f",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "### Task 1.1\n",
    "Can you visualize the corresponding `mask` for the image above? <br>\n",
    "(*Hint*: Look for the same name within the `masks` directory.) <br>\n",
    "What does the mask show?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd7b41b3",
   "metadata": {
    "tags": [
     "task"
    ]
   },
   "outputs": [],
   "source": [
    "##########################\n",
    "######## To Do ###########\n",
    "##########################\n",
    "\n",
    "mask = ...  # TODO\n",
    "print(f\"Mask `mask` has type {type(mask)}\")  # variable type\n",
    "plt.imshow(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51d16a13",
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "##########################\n",
    "####### Solution #########\n",
    "##########################\n",
    "\n",
    "mask = imread(\"monuseg-2018/download/masks/TCGA-2Z-A9J9-01A-01-TS1.tif\")\n",
    "print(f\"Mask `mask` has type {type(mask)}\")  # variable type\n",
    "plt.imshow(mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e92d7621",
   "metadata": {},
   "source": [
    "### Function description\n",
    "There is a way in python to get the description of a function. This is useful when you are not sure what the function does or what arguments it takes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "339fed7c",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "### Task 1.2\n",
    "\n",
    "Try to get the description of the `imread` function that we used above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73f9fd12",
   "metadata": {
    "tags": [
     "task"
    ]
   },
   "outputs": [],
   "source": [
    "##########################\n",
    "######## To Do ###########\n",
    "##########################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f5648d4",
   "metadata": {
    "lines_to_next_cell": 0,
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "##########################\n",
    "####### Solution #########\n",
    "##########################\n",
    "\n",
    "imread?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb61830c",
   "metadata": {},
   "source": [
    "### Image channels\n",
    "If the image is a `grayscale` image, then the number of channels is equal to $1$,\n",
    "in which case the array can also be of shape (height, width). <br>\n",
    "If the image is `RGB`, then the number of channels is $3$.\n",
    "with each channel encoding the red, green and blue components."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59a03ae9",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "### Task 1.3\n",
    "Is <code>img</code> RGB or grayscale ? What about the mask?\n",
    "\n",
    "*Hint*: <a href=\"https://assets.datacamp.com/blog_assets/Numpy_Python_Cheat_Sheet.pdf\">np cheatsheet</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d04ad1",
   "metadata": {
    "tags": [
     "task"
    ]
   },
   "outputs": [],
   "source": [
    "##########################\n",
    "######## To Do ###########\n",
    "##########################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24430983",
   "metadata": {
    "lines_to_next_cell": 0,
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "##########################\n",
    "######## Solution ###########\n",
    "##########################\n",
    "\n",
    "print(img.shape)\n",
    "print(mask.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "050c25f3",
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "source": [
    "```\n",
    "##########################\n",
    "####### Solution #########\n",
    "##########################\n",
    "```\n",
    "The shape of the np array can tell us if the image is RGB or grayscale.\n",
    "If the shape is (height, width, 3), then the image is RGB.\n",
    "If the shape is (height, width), then the image is grayscale. Therefore:\n",
    "- Mask = greyscale\n",
    "- Image = RGB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbf55797",
   "metadata": {},
   "source": [
    "### Image data types\n",
    "\n",
    "\n",
    "Images can be represented by a variety of data types. The following is a list of the most common datatypes:\n",
    "- `bool`: binary, 0 or 1\n",
    "- `uint8`: unsigned integers, 0 to 255 range\n",
    "- `float`: -1 to 1 or 0 to 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fc3b809",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "### Task 1.4\n",
    "What is the data type of <code>img</code> and the <code>mask</code> ? What are the minimum and maximum intensity values?\n",
    "\n",
    "*Hint*: <a href=\"https://assets.datacamp.com/blog_assets/Numpy_Python_Cheat_Sheet.pdf\">np cheatsheet</a></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11684732",
   "metadata": {
    "tags": [
     "task"
    ]
   },
   "outputs": [],
   "source": [
    "##########################\n",
    "######## To Do ###########\n",
    "##########################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59486dbf",
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "##########################\n",
    "####### Solution #########\n",
    "##########################\n",
    "\n",
    "print(\"data type: \", img.dtype, mask.dtype)\n",
    "print(\"Image min and max: \", img.min(), img.max())\n",
    "print(\"Mask min and max: \", mask.min(), mask.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc47f6ce",
   "metadata": {},
   "source": [
    "### Reshaping Images\n",
    "\n",
    "`PyTorch`, `TensorFlow` and `JAX` are popular deep learning frameworks.\n",
    "<br> In `PyTorch` images are represented as (`num_channels`, `height`, `width`).\n",
    "\n",
    "But the image which we are working with has the `channel` as the last axis.\n",
    "Therefore, we need to reshape (by swapping) the image to the correct shape."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3b71435",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "### Task 1.5\n",
    "Reshape <code>img</code> such that its shape is <code>(num_channels, height, width)</code>\n",
    "\n",
    "*Hint*: <a href=\"https://numpy.org/doc/stable/reference/generated/numpy.transpose.html\">np transpose</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13eb831a",
   "metadata": {
    "tags": [
     "task"
    ]
   },
   "outputs": [],
   "source": [
    "##########################\n",
    "######## To Do ###########\n",
    "##########################\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "print(f\"Before reshaping, image has shape {img.shape}\")\n",
    "img_reshaped = ...  ## TODO\n",
    "print(f\"After reshaping, image has shape {img_reshaped.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "876e97fd",
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "##########################\n",
    "####### Solution #########\n",
    "##########################\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "print(f\"Before reshaping, image has shape {img.shape}\")\n",
    "img_reshaped = np.transpose(img, (2, 0, 1))\n",
    "print(f\"After reshaping, image has shape {img_reshaped.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61925783",
   "metadata": {},
   "source": [
    "### Manipulating dimensions\n",
    "\n",
    "Manipulating dimensions is a common operation in deep learning. <br>\n",
    "Sometimes, the a neural network expects the input to be in a certain format, and we need to reshape the data accordingly. <br>\n",
    "Lets add a batch dimension (batching will be introduced later) to the image. <br>\n",
    "We can use `np.newaxis` to add a new axis to the image for this case as we are dealing with numpy arrays.\n",
    "Later we will see how to do this in PyTorch using `torch.unsqueeze`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "646ea236",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_with_batch = img_reshaped[np.newaxis, ...] #here we are adding a new axis at the 0th position, which is the batch dimension `...` means all the other dimensions\n",
    "print(f\"Image with batch has shape {img_with_batch.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "946e2240",
   "metadata": {},
   "source": [
    "### Normalizing Images\n",
    "\n",
    "Providing image inputs with intensities between [0, 1] often helps when training the model. <br>\n",
    "This is because the gradients are more stable and the model converges faster, and the model is not biased towards any particular intensity values. <br>\n",
    "One way of normalizing an image is to divide the intensity on each pixel by the maximum allowed intensity for the available data type."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e05b0a",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 0
   },
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "### Task 1.6\n",
    "Obtain an intensity normalized image using the idea above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8532ca7",
   "metadata": {
    "lines_to_next_cell": 2,
    "tags": [
     "task"
    ]
   },
   "outputs": [],
   "source": [
    "##########################\n",
    "######## To Do ###########\n",
    "##########################\n",
    "\n",
    "\n",
    "def normalize(img):\n",
    "    norm_img = ...  # TODO\n",
    "    return norm_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7698c627",
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "##########################\n",
    "####### Solution #########\n",
    "##########################\n",
    "\n",
    "\n",
    "def normalize(img):\n",
    "    norm_img = img / 255\n",
    "    return norm_img"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adfb4b9b",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "### Task 1.7\n",
    "What is the data type of the normalized image. Has it changed from before? Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a35a05",
   "metadata": {
    "tags": [
     "task"
    ]
   },
   "outputs": [],
   "source": [
    "##########################\n",
    "######## To Do ###########\n",
    "##########################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa3be99",
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "##########################\n",
    "####### Solution #########\n",
    "##########################\n",
    "\n",
    "normalize(img).dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdcc423e",
   "metadata": {},
   "source": [
    "### Loading a set of images\n",
    "\n",
    "Given a set of images in a folder, we need to be able to easily find the pathnames and load them in. <br>\n",
    "`glob` is a standard package that provides a utility for finding all pathnames that match a given pattern.\n",
    "\n",
    "Here, our images have the `.tif` extension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae77f1d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "\n",
    "img_dir = \"monuseg-2018/download/images/\"\n",
    "img_filenames = sorted(glob(os.path.join(img_dir, \"*.tif\")))\n",
    "\n",
    "print(f\"Found:\")\n",
    "for img_filename in img_filenames:\n",
    "    print(f\"{img_filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "513ac3d2",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "### Task 1.8\n",
    "Load the set of masks, by correctly specifying the value of the variables `mask_dir` and `mask_filenames`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67aeee9e",
   "metadata": {
    "tags": [
     "task"
    ]
   },
   "outputs": [],
   "source": [
    "##########################\n",
    "######## To Do ###########\n",
    "##########################\n",
    "\n",
    "mask_dir = ...  # TODO: fill value!!\n",
    "mask_filenames = ...  # TODO: fill value!!\n",
    "for mask_filename in mask_filenames:\n",
    "    print(f\"{mask_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a6b61c3",
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "##########################\n",
    "####### Solution #########\n",
    "##########################\n",
    "\n",
    "mask_dir = \"monuseg-2018/download/masks/\"\n",
    "mask_filenames = sorted(glob(os.path.join(mask_dir, \"*.tif\")))\n",
    "for mask_filename in mask_filenames:\n",
    "    print(f\"{mask_filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5914b31",
   "metadata": {},
   "source": [
    "Let's visualize some of the images and the corresponding mask, side by side. First let's provide a helper `visualize` function which takes two images as argument.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f91bddd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def visualize(im1, im2):\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.subplot(121)\n",
    "    plt.imshow(im1)\n",
    "    plt.subplot(122)\n",
    "    plt.imshow(im2)\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14e21226",
   "metadata": {},
   "source": [
    "Executing the cell below, would visualize a new random image and the corresponding segmentation mask, each time. This is because the variable `idx` gets a new value between $0$ and $14$ (there are $15$ images)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5dcad29",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 8 #change this value to visualize a different image and mask to explore the dataset\n",
    "visualize(imread(img_filenames[idx]), imread(mask_filenames[idx]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df4ab1ee",
   "metadata": {},
   "source": [
    "Note that all the labels in the right image have different colors. This is because the mask is a instance mask where each color represents a different class.\n",
    "The colors indicate an unique id for each object in the image."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e8272b6",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 0
   },
   "source": [
    "<div class=\"alert alert-success\">\n",
    "\n",
    "## Checkpoint 1\n",
    "\n",
    "Great Job! 🎊 Please post in the chat when you reach this checkpoint.\n",
    "\n",
    "In the first chapter, we learned about:\n",
    "\n",
    "<li> image data types</li>\n",
    "<li> reshaping images </li>\n",
    "<li> normalizing images </li>\n",
    "<li> Using <code>glob</code> to load a set of images\n",
    "\n",
    "These are important concepts to understand as they form the basis of most image processing pipelines because they are the basic data handling operations.\n",
    "\n",
    "<hr>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8127bd9",
   "metadata": {},
   "source": [
    "**Bonus Task for Chapter 1**: Can you think of alternate approaches to intensity normalization? Any benefits of following one over the other?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9a025de",
   "metadata": {},
   "source": [
    "Another approach could be percentile normalization. This maps the min intensity to 0 and max intensity to 0. This is useful if there are\n",
    "outliers in the data. The benefit of this approach is that it is robust to outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbf8fd79",
   "metadata": {},
   "source": [
    "## Chapter 2\n",
    "### Cropping\n",
    "\n",
    "While training models, we usually feed in smaller crops extracted from the original images.\n",
    "To do so, we can rely on the powerful numpy [indexing](https://numpy.org/doc/stable/user/basics.indexing.html).\n",
    "\n",
    "For example, let's extract the top left (second) quadrant from one of our images.\n",
    "\n",
    "In the cell below, the original image is visualized on the left and the cropped image is seen on the right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d866b70d",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = np.random.randint(len(img_filenames))\n",
    "img = imread(img_filenames[idx])\n",
    "cropped_img = img[0:500, 0:500, :]\n",
    "visualize(img, cropped_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0dacbc9",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "### Task 2.1\n",
    "Visualize the bottom left (third) quadrant of a random image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0033f853",
   "metadata": {
    "tags": [
     "task"
    ]
   },
   "outputs": [],
   "source": [
    "##########################\n",
    "######## To Do ###########\n",
    "##########################\n",
    "\n",
    "idx = np.random.randint(len(img_filenames))\n",
    "img = imread(img_filenames[idx])\n",
    "cropped_img = ...  # TODO : fill correct value!!\n",
    "visualize(img, cropped_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bbfb53c",
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "##########################\n",
    "####### Solution #########\n",
    "##########################\n",
    "\n",
    "idx = np.random.randint(len(img_filenames))\n",
    "img = imread(img_filenames[idx])\n",
    "cropped_img = img[500:, 0:500, :]\n",
    "visualize(img, cropped_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b25fa272",
   "metadata": {},
   "source": [
    "### Downsampling\n",
    "\n",
    "For large images, sometimes we require that they are downsampled to fit in memory. <br>\n",
    "\n",
    "**Note:** Downsampling is a lossy operation, as we are removing information from the image. If your images contain important information in the removed pixels, this can be detrimental to the model's performance. <br>\n",
    "This is usually not a problem for images with a high resolution, as the removed information is often redundant. <br>\n",
    "However, if subtle structes in the content of your image is important, it is better to crop the image rather than downsample it. <br>\n",
    "\n",
    "Say if one wants to have every fourth pixel from the original image, one specifies `factor` = $4$, and one can run the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e4aabc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# downsampling\n",
    "idx = np.random.randint(len(img_filenames))\n",
    "img = imread(img_filenames[idx])\n",
    "\n",
    "factor = 4\n",
    "downsampled_img = img[::factor, ::factor] # here we are selecting every 'factor' pixel in the height and width dimension\n",
    "print(f\"Original image shape: {img.shape}\")\n",
    "print(f\"Downsampled image shape: {downsampled_img.shape}\")\n",
    "\n",
    "# Let's visualize the original image and the downsampled image side by side\n",
    "visualize(img, downsampled_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa9238eb",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "### Task 2.2\n",
    "Can you see that the image on the right lacks some detail on account of being downsampled? <br>\n",
    "This might not be readily evident now, but if you look closely, you can see that the image on the right is a bit blurry. <br>\n",
    "This is because we are skipping pixels while downsampling.\n",
    "\n",
    "To make the effect more evident, try other extreme values of the downsampling `factor`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad9186fd",
   "metadata": {
    "tags": [
     "task"
    ]
   },
   "outputs": [],
   "source": [
    "##########################\n",
    "######## To Do ###########\n",
    "##########################\n",
    "\n",
    "factor = ...\n",
    "downsampled_img = img[::factor, ::factor]\n",
    "visualize(img, downsampled_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69ec734d",
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "##########################\n",
    "####### Solution #########\n",
    "##########################\n",
    "\n",
    "factor = 8\n",
    "downsampled_img = img[::factor, ::factor]\n",
    "visualize(img, downsampled_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25a7d966",
   "metadata": {},
   "source": [
    "### Flipping\n",
    "\n",
    "Sometimes, one wishes to create new images from original data by performing transformations.\n",
    "\n",
    "One way to create a new image is by flipping an image about a given axis, which creates a mirror image!\n",
    "\n",
    "Run the following cell to visualize a vertically flipped image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "709189d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = np.random.randint(len(img_filenames))\n",
    "img = imread(img_filenames[idx])\n",
    "# Here the image dimensions are (height, width, num_channels), ::-1 means reverse the order of the elements in the array on the height axis\n",
    "vflipped_img = img[::-1, :, :]\n",
    "visualize(img, vflipped_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0722f02b",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "### Task 2.3\n",
    "Create a horizontally flipped image and visualize!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e696273f",
   "metadata": {
    "tags": [
     "task"
    ]
   },
   "outputs": [],
   "source": [
    "##########################\n",
    "######## To Do ###########\n",
    "##########################\n",
    "\n",
    "idx = np.random.randint(len(img_filenames))\n",
    "img = imread(img_filenames[idx])\n",
    "hflipped_img = ...  ## TODO: fill correct value\n",
    "visualize(img, hflipped_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b2d4cdb",
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "##########################\n",
    "####### Solution #########\n",
    "##########################\n",
    "\n",
    "idx = np.random.randint(len(img_filenames))\n",
    "img = imread(img_filenames[idx])\n",
    "hflipped_img = img[:, ::-1, :]\n",
    "visualize(img, hflipped_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75b96a31",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 0
   },
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "### Bonus Task for Chapter 2\n",
    "Can you think of any other reason why we need to crop images, aside from saving memory?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17fea0af",
   "metadata": {
    "tags": [
     "task"
    ]
   },
   "outputs": [],
   "source": [
    "##########################\n",
    "######## To Do ###########\n",
    "##########################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55f83227",
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "source": [
    "\n",
    "```\n",
    "##########################\n",
    "####### Solution #########\n",
    "##########################\n",
    "```\n",
    "\n",
    "Answer: Because often the training images come with different sizes and this creates an issue while `batching` the images."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6a39425",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "<div class=\"alert alert-success\">\n",
    "\n",
    "<hr>\n",
    "Fantastic Work! 🙏 Please post on the course chat when you reach this checkpoint.\n",
    "\n",
    "## Checkpoint 2\n",
    "\n",
    "In the second chapter, we learnt about:\n",
    "\n",
    "<li> cropping images </li>\n",
    "<li> downsampling images </li>\n",
    "<li> flipping images </li>\n",
    "\n",
    "These operations are important as they help in creating new images from the original data.\n",
    "These are also ways of basic data augmentation which is useful for training models.\n",
    "\n",
    "<hr>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c07de73",
   "metadata": {},
   "source": [
    "**Bonus Task for Chapter 2**\n",
    "\n",
    "Can you think of reasons why we need to crop images?\n",
    "Can't we feed in all the images at the original size to the model?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec3c0cf5",
   "metadata": {},
   "source": [
    "1. To increase the amount of data i.e, have more training data and corresponding masks\n",
    "2. Also because often the training images come with different sizes and this creates an issue while `batching` the images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5e08fef",
   "metadata": {},
   "source": [
    "## Chapter 3\n",
    "### Batching\n",
    "\n",
    "In ML/DL, we often have to deal with very large datasets. It soon becomes inefficient to process all the data at once, so it's useful to split the data into \"mini-batches\" that we can process individually.\n",
    "\n",
    "So for purely reasons of computational cost, this is often useful.\n",
    "\n",
    "We will also see another reason for which batching can be useful - for instance when running gradient descent on non-convex landscapes.\n",
    "Here, computing the gradient on a subset of the data gives us an approximate/noisy gradient making it less likely for us to end up being stuck in local minima. This is what is referred to as \"stochastic gradient descent\".\n",
    "\n",
    "Let us make our first batch of images, containing $B$ number of images.\n",
    "The shape of the batch will thus get an additional \"batch dimension\" at the first dimension, i.e. (batch_size, num_channels, height, width)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cccc9478",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "### Task 3.1\n",
    "\n",
    "Make a batch of size $B=4$ by sampling 4 images randomly from the available images (this will be a 4D np array).\n",
    "<br> Here, you would also have to ensure that the second axis corresponds to the channel (use `np.transpose`)\n",
    "\n",
    "**Hint**: Use `np.random.choice` to sample 4 random indices from the list of image filenames. \n",
    "`np.random.randint` (which we have used above) to select batches could give duplicate images in a batch. In this case, your batch should contain 4 unique images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa9f6e3d",
   "metadata": {
    "tags": [
     "task"
    ]
   },
   "outputs": [],
   "source": [
    "##########################\n",
    "######## To Do ###########\n",
    "##########################\n",
    "\n",
    "# `batch` should be a np array with shape (4, 3, 1000, 1000).\n",
    "\n",
    "batch = ...  # TODO\n",
    "print(f\"Batch of images has shape {batch.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ba116e9",
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "##########################\n",
    "####### Solution #########\n",
    "##########################\n",
    "\n",
    "indices = np.random.choice(len(img_filenames), 4, replace=False)\n",
    "imgs = []\n",
    "for index in indices:\n",
    "    imgs.append(np.transpose(imread(img_filenames[index]), (2, 0, 1)))\n",
    "batch = np.asarray(imgs)\n",
    "print(f\"Batch of images has shape {batch.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8682bf47",
   "metadata": {},
   "source": [
    "### Convolutions\n",
    "\n",
    "Convolutions are the elementary operations used in Convolutional Neural Networks (CNNs). <br> The images (and later, the feature maps) are convolved with multiple filters whose weights are learned. <br>\n",
    "\n",
    "![](https://upload.wikimedia.org/wikipedia/commons/1/19/2D_Convolution_Animation.gif)\n",
    "\n",
    "\n",
    "Please read this section https://en.wikipedia.org/wiki/Kernel_(image_processing)#Convolution on convolutions to learn how to implement a your own convolution function!\n",
    "\n",
    "Here we will optionally demonstrate TQDM which is a package that provides a progress bar for loops.\n",
    "This is useful when you have a loop that takes a long time to run and you want to know how far along the loop is.\n",
    "Read more about TQDM [here](https://tqdm.github.io/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63a1998c",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "### Task 3.2\n",
    "Implement a function that performs a convolution of an image with a filter.\n",
    "<br> Assume that your image is square and that your filter is square and has an odd width. You can set arbitrary values in your filter for now.\n",
    "<br> Note that your output image will be smaller.\n",
    "\n",
    "**A few things to note:** <br>\n",
    "-- Filter: A filter is a matrix that is convolved with the image. The filter is also known as a kernel. It computes the correlation between the itself and the image. It is kind of finding the similarity between the filter and the image.\n",
    "The filter is applied to the image by sliding the filter over the image and computing the dot product between the filter and the image at each location. The output of the convolution is called the feature map (a concept that will come up in future exercises).\n",
    "\n",
    "-- The filter is usually a square matrix with odd dimensions. This is because the filter is applied to the image by sliding it over the image. If the filter has even dimensions, it will not have a center pixel. This will make it difficult to align the filter with the image. \n",
    "If the filter is not square, we loose the symmetrical applicability of the filter. The filter then will have to be applied in different directions (height, width) separately. This will make the implementation more complex and migt lead to distortions in the output features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8cf3375",
   "metadata": {
    "tags": [
     "task"
    ]
   },
   "outputs": [],
   "source": [
    "##########################\n",
    "######## To Do ###########\n",
    "##########################\n",
    "\n",
    "from tqdm import tqdm # tqdm is used to show progress bars in loops\n",
    "\n",
    "def conv2d(img, kernel):\n",
    "    assert kernel.shape[0] == kernel.shape[1]\n",
    "    assert kernel.shape[0] % 2 != 0\n",
    "\n",
    "    h, w = img.shape[0], img.shape[1]  # Starting size of image\n",
    "    d_k = kernel.shape[0]  # Size of kernel\n",
    "\n",
    "    h_new = h - d_k + 1  # Calculate the new height of the array\n",
    "    w_new = w - d_k + 1  # Calculate the new width of the array\n",
    "    output = np.zeros((h_new, w_new))\n",
    "\n",
    "    # TODO: add your code for filling output with the convolved image\n",
    "    for i in tqdm(range(output.shape[0]), desc=\"Processing rows\", position=0, leave=True):\n",
    "        for j in tqdm(range(output.shape[1]),desc=\"Processing columns\", position=0, leave=False):\n",
    "            ...\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd0983cf",
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "##########################\n",
    "####### Solution #########\n",
    "##########################\n",
    "\n",
    "from tqdm import tqdm # tqdm is used to show progress bars in loops\n",
    "\n",
    "def conv2d(img, kernel):\n",
    "    assert kernel.shape[0] == kernel.shape[1]\n",
    "    assert kernel.shape[0] % 2 != 0\n",
    "\n",
    "    h, w = img.shape[0], img.shape[1]  # Starting size of image\n",
    "    d_k = kernel.shape[0]  # Size of kernel\n",
    "\n",
    "    h_new = h - d_k + 1\n",
    "    w_new = w - d_k + 1\n",
    "    output = np.zeros((h_new, w_new))\n",
    "\n",
    "    for i in tqdm(range(output.shape[0]), desc=\"Processing rows\", position=0, leave=True):\n",
    "        for j in tqdm(range(output.shape[1]),desc=\"Processing columns\", position=0, leave=False):\n",
    "            output[i, j] = np.sum(img[i:i + d_k, j:j + d_k] * kernel)\n",
    "            pbar.update(1)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b3acac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to check your function\n",
    "\n",
    "identity = np.array([[0, 0, 0], \n",
    "                     [0, 1, 0], \n",
    "                     [0, 0, 0]])\n",
    "# Let's take a 256x256 center crop of the image for better visualization of the effect of the convolution\n",
    "new_im = conv2d(img[128:384, 128:384, 0], identity)\n",
    "# Lets print the original image and the convolved image\n",
    "print(img[128:384, 128:384, 0].shape)\n",
    "print(new_im.shape)\n",
    "\n",
    "#Lets visualize the original image and the convolved image and the filter\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.subplot(131)\n",
    "plt.imshow(img[128:384, 128:384, 0])\n",
    "plt.title(\"Original Image\")\n",
    "plt.subplot(132)\n",
    "plt.imshow(identity)\n",
    "plt.title(\"Kernel\")\n",
    "plt.subplot(133)\n",
    "plt.imshow(new_im)\n",
    "plt.title(\"Convolved Image\")\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a4eb615",
   "metadata": {},
   "source": [
    "**Bonus: Try differnt (arbitary?) filters and see how the output changes! You can find some here: https://en.wikipedia.org/wiki/Kernel_(image_processing)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d4628c8",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 0
   },
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "### Task 3.3\n",
    "\n",
    "We noticed that the output image is smaller than the input image! <br>\n",
    "\n",
    "Can you come up with an analytical relationship regarding how much smaller the output image is *vis-à-vis* the input image? <br>\n",
    "Can you think of any strategy which ensures that the output image is the same size as the input image?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5023e78c",
   "metadata": {
    "tags": [
     "task"
    ]
   },
   "outputs": [],
   "source": [
    "##########################\n",
    "######## To Do ###########\n",
    "##########################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2fdae51",
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "source": [
    "\n",
    "```\n",
    "##########################\n",
    "####### Solution #########\n",
    "##########################\n",
    "```\n",
    "\n",
    "- Given an input image of size $H \\times W$, a filter of size $K_h \\times K_w$ , and strides $S_h$ and $S_w$\n",
    "the output size (height $H_{out}$ and width $W_{out}$) can be calculated using the following formulas (Note that $\\lfloor.\\rfloor$ is the floor operator):\n",
    "\n",
    "$$\n",
    "\\begin{equation*}\n",
    "    H_{out} = \\left\\lfloor \\frac{H - K_h}{S_h} \\right\\rfloor + 1\n",
    "\\end{equation*}\n",
    "$$\n",
    "$$\n",
    "\\begin{equation*}\n",
    "    W_{out} = \\left\\lfloor \\frac{W - K_w}{S_w} \\right\\rfloor + 1\n",
    "\\end{equation*}\n",
    "$$\n",
    "- We can add padding to the input image to ensure that the output image is the same size as the input image."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38ea51b3",
   "metadata": {},
   "source": [
    "### Filters\n",
    "\n",
    "Let us try to understand what could the values of the filter be. <br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa90d6a4",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "The following is known as the [Sobel filter](https://en.wikipedia.org/wiki/Sobel_operator):\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "    1 & 2 & 1 \\\\\n",
    "    0 & 0 & 0 \\\\\n",
    "    -1 & -2 & -1\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "### Task 3.4\n",
    "Apply the Sobel filter and describe what it does"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6341dcad",
   "metadata": {
    "tags": [
     "task"
    ]
   },
   "outputs": [],
   "source": [
    "##########################\n",
    "######## To Do ###########\n",
    "##########################\n",
    "\n",
    "flter = ...  # TODO\n",
    "output_img = conv2d(img[128:384, 128:384, 0], flter)\n",
    "visualize(img[128:384, 128:384, 0], output_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e299e653",
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "##########################\n",
    "####### Solution #########\n",
    "##########################\n",
    "\n",
    "flter = np.array([[1, 2, 1], \n",
    "                  [0, 0, 0], \n",
    "                  [-1, -2, -1]])\n",
    "output_img = conv2d(img[128:384, 128:384, 0], flter)\n",
    "visualize(img[128:384, 128:384, 0], output_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c7934d7",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "### Task 3.5\n",
    "\n",
    "What feature in this image do you think this filter is detecting?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cbad60e",
   "metadata": {
    "tags": [
     "task"
    ]
   },
   "outputs": [],
   "source": [
    "##########################\n",
    "######## To Do ###########\n",
    "##########################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6f3f302",
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "source": [
    "```\n",
    "##########################\n",
    "####### Solution #########\n",
    "##########################\n",
    "```\n",
    "\n",
    "The Sobel filter is detecting the edges in the image."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5787692",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "<div class=\"alert alert-success\">\n",
    "\n",
    "<hr>\n",
    "Wow! 🤟 Post on the course chat when you reach this checkpoint!\n",
    "\n",
    "## Checkpoint 3\n",
    "\n",
    "In the third chapter, we learnt about:\n",
    "\n",
    "<li> Batching (here we understood the importance of batching in training models) </li>\n",
    "<li> Convolutions (The primary operation in Convolutional Neural Networks) </li>\n",
    "<li> Filters (The weights that are learned in Convolutional Neural Networks) </li>\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ff29c33",
   "metadata": {},
   "source": [
    "## Chapter 4: Data augmentation\n",
    "\n",
    "Having collected your hard earned data you want to make the most of it. In ML/DL, we're often limited by the size of our the training set. \n",
    "How could we artificially increase our data to provide more input to our model and help it generalize better?\n",
    "\n",
    "One trick is to make simple transformations to our data such as rotating or flipping it - this process is generally called \"data augmentation\" (DA) and is widely used in ML.\n",
    "\n",
    "Here we will implement some basic data augmentation techniques using numpy. \n",
    "\n",
    "**Note:** This is a good opportunity to learn how this augmentations work under the hood. \n",
    "However in practice, we would use libraries like `torchvision` or `monai` which are optimized for speed and have a wide range of augmentations available."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e237425c",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "### Applying one augmentation at a time\n",
    "Lets apply one augmentation to the image at a time.\n",
    "**Note:** With advanced libraries like `torchvision` or `monai`, you can apply multiple augmentations at once. [Read more about [Torchvision](https://pytorch.org/vision/stable/index.html) and [Monai](https://docs.monai.io/en/latest/transforms.html)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9446d53",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "# Flip horizontally\n",
    "img_flip_horizontal = img[:, ::-1]\n",
    "# Flip vertically\n",
    "img_flip_vertical = img[::-1, :]\n",
    "# Rotate by 45 degrees\n",
    "img_rotate = np.rot90(img, k=1)\n",
    "\n",
    "#Let's visualize the original image and the augmented image\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.subplot(141)\n",
    "plt.imshow(img)\n",
    "plt.title(\"Original Image\")\n",
    "plt.subplot(142)\n",
    "plt.imshow(img_flip_horizontal)\n",
    "plt.title(\"Horizontal Flip\")\n",
    "plt.subplot(143)\n",
    "plt.imshow(img_flip_vertical)\n",
    "plt.title(\"Vertical Flip\")\n",
    "plt.subplot(144)\n",
    "plt.imshow(img_rotate)\n",
    "plt.title(\"Rotate by 45 degrees\")\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c93edde",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "### Task 4.1\n",
    "Familiarize yourself with the different augmentations available through `torchvision`.\n",
    "\n",
    "Refer to the [examples](https://pytorch.org/vision/0.13/transforms.html) and identify and apply augmentations that you think are interesting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4367eec3",
   "metadata": {
    "tags": [
     "task"
    ]
   },
   "outputs": [],
   "source": [
    "##########################\n",
    "######## To Do ###########\n",
    "##########################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "150d2b0d",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "### Task 4.2\n",
    "Q1. While augmenting images and segmentation masks, should they be augmented similarly or differently? Discuss. <br>\n",
    "Q2. What are the disadvantages of augmenting images? When can it be harmful?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5486a05e",
   "metadata": {
    "tags": [
     "task"
    ]
   },
   "outputs": [],
   "source": [
    "##########################\n",
    "######## To Do ###########\n",
    "##########################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "228374c5",
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "source": [
    "```\n",
    "##########################\n",
    "####### Solution #########\n",
    "##########################\n",
    "```\n",
    "\n",
    "A1.  The same geometric transform should be applied to both the images and the corresponding segmentation masks. <br>\n",
    "A2. Augmenting images can be computationally expensive and can lead to overfitting if not done properly. It can be harmful when the augmentations are not appropriate for the task at hand. For example, flipping a medical image horizontally can lead to incorrect diagnosis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b694823",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "<div class=\"alert alert-success\">\n",
    "\n",
    "<hr>\n",
    "Hurrah! 😃\n",
    "\n",
    "## Checkpoint 4\n",
    "\n",
    "In the fourth chapter, we learnt about data augmentation which is an important concept in training models. Data augmentation helps in artificially increasing the size of the training data which is useful for training models in a data efficient way.\n",
    "We implemented some basic data augmentation techniques using numpy.\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40e21fa0",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "## Chapter 5: (Optional) Advanced Plotting and Visualization\n",
    "\n",
    "Python's matplotlib is a powerful library for creating visualizations.\n",
    "It is also highly customizable and can be used to create complex plots.\n",
    "In deep learning, visualizing the data is important as it helps in understanding the data and the model better.\n",
    "Therefore, it is important to know how to create different types of plots using matplotlib with specific (useful) functions.\n",
    "\n",
    "### Colormap\n",
    "You can choose a specific colormap to visualize your images.\n",
    "Read more about colormaps [here](https://matplotlib.org/stable/tutorials/colors/colormaps.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b13a776",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(img[..., 0], cmap=\"magma\")  #Note that we are only showing the first channel of the image\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fa8e541",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "\n",
    "### Colorbar\n",
    "Sometimes, you may want to add a colorbar to your image to show the intensity values.\n",
    "\n",
    "**Note**: The <code>vmin</code> and <code>vmax</code> arguments are used to set the range of the colorbar.\n",
    "In this case, the intensity values range from 0 to 255. But you can set the range to any values you want (based on the intensity values in the image). <br>\n",
    "**Note**: for plotting colorbar for instance mask (in the future), you can use <code>viridis</code> colormap for the mask for better visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "715c2def",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 6))\n",
    "plt.imshow(img[..., 0], cmap=\"magma\", vmin=0, vmax=255)\n",
    "plt.colorbar()\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed217db8",
   "metadata": {},
   "source": [
    "\n",
    "### Plotting Histograms\n",
    "- Let's see the histogram of the intensity values\n",
    "- <code>img</code> is an 8-bit image, so the intensity values range from 0 to 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bd2b429",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 6))\n",
    "# bins indicate how may unique buclets you might want to put your values in and range is the min and max values in the image\n",
    "# Try different values for bins and range to see how the histogram changes\n",
    "plt.hist(img[..., 0].ravel(), bins=255, range=(0.0, img[..., 0].max()))\n",
    "plt.xlabel(\"Intensity Value\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d61467f",
   "metadata": {},
   "source": [
    "Often we might need to plot multiple images side by side.\n",
    "Let's plot the crop of the original image from different locations side by side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66f1a589",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, 2, figsize=(8, 8))\n",
    "ax[0, 0].imshow(img[0:512, 0:512, :], cmap=\"magma\")\n",
    "ax[0, 1].imshow(img[512:, 512:, :], cmap=\"magma\")\n",
    "ax[1, 0].imshow(img[512:, 0:512, :], cmap=\"magma\")\n",
    "ax[1, 1].imshow(img[0:512, 512:, :], cmap=\"magma\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c2fc620",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 0
   },
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "### Task 5.1\n",
    "With all that knowledge, let's plot the 4 crop of the image in a first row and their histograms in a second row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe9614db",
   "metadata": {
    "tags": [
     "task"
    ]
   },
   "outputs": [],
   "source": [
    "##########################\n",
    "######## To Do ###########\n",
    "##########################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0564af8",
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "##########################\n",
    "####### Solution #########\n",
    "##########################\n",
    "\n",
    "#make a list of the 4 images\n",
    "images = [img[0:512, 0:512, :], \n",
    "          img[512:, 512:, :], \n",
    "          img[512:, 0:512, :], \n",
    "          img[0:512,512:, :]]\n",
    "fig, ax = plt.subplots(2, 4, figsize=(25, 10))\n",
    "for i in range(0, 4):\n",
    "    ax[0, i].imshow(images[i], cmap=\"magma\")\n",
    "    ax[0, i].set_title(\"Image \" + str(i))\n",
    "    ax[1, i].hist(images[i].ravel(), bins=255, range=(0.0, 255.0))\n",
    "    ax[1, i].set_xlabel(\"Intensity Value\")\n",
    "    ax[1, i].set_ylabel(\"Count\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dbe73ac",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "<div class=\"alert alert-success\">\n",
    "\n",
    "<hr>\n",
    "Hurrah! 😃\n",
    "\n",
    "## Checkpoint 5\n",
    "\n",
    "Chapter 5 is a bonus chapter where we learnt about:\n",
    "\n",
    "<li> Using colormaps to visualize images </li>\n",
    "<li> Adding colorbars to images </li>\n",
    "<li> Plotting histograms </li>\n",
    "<li> Plotting multiple images side by side </li>\n",
    "<hr>"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "all",
   "main_language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
