{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d92a79cf",
   "metadata": {},
   "source": [
    "# Python Boot Camp\n",
    "\n",
    "\n",
    "\n",
    "Welcome! ðŸ˜ƒðŸ‘‹\n",
    "\n",
    "In this notebook, we will go through a comprehensive Python boot camp focused on foundational computer vision concepts. You will learn about:\n",
    "\n",
    "* **Preparing and understanding image data**: How to programmatically download a dataset, inspect image properties like shape and data type, and preprocess them using `tifffile` and `numpy`.\n",
    "* **Data augmentation**: Implementing common image transformations such as flipping and rotation using both basic Python libraries like `numpy` and deep-learning frameworks like PyTorch's `torchvision`.\n",
    "* **Convolutions**: Understanding and implementing 2D convolutions, the foundational operation of Convolutional Neural Networks (CNNs), and seeing how different filters can extract specific features from an image.\n",
    "* **Efficient data loading**: Creating batches of data and building an efficient data loading pipeline using PyTorch's `Dataset` and `DataLoader` classes.\n",
    "* **Advanced image analysis**: Using libraries like `scikit-image` and `matplotlib` to perform analyses, such as visualizing cell size distributions and overlaying segmentation masks on original images.\n",
    "\n",
    "We will be using sample images from the *MoNuSeg* dataset provided by [Kumar et al, 2018](https://ieeexplore.ieee.org/document/8880654). The data was publicly made available [here](https://monuseg.grand-challenge.org/) by the authors of the publication. This dataset shows Hematoxylin and Eosin (H&E) Stained Images showing nuclei in different shapes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "658d2fb6",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "## Chapter 0: Downloading the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d299cc4",
   "metadata": {},
   "source": [
    "Let us first download the images from an external url, which is a zip file containing the dataset.\n",
    "We will then extract the zip file to a folder named `monuseg-2018`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7f09c80",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import urllib.request, zipfile\n",
    "\n",
    "\n",
    "def extract_data(zip_url, project_name):\n",
    "    zip_path = Path(project_name + \".zip\")\n",
    "    if zip_path.exists():\n",
    "        print(\"Zip file was downloaded and extracted before!\")\n",
    "    else:\n",
    "        urllib.request.urlretrieve(zip_url, zip_path)\n",
    "        print(\"Downloaded data as {}\".format(zip_path))\n",
    "    with zipfile.ZipFile(zip_path, \"r\") as zip_ref:\n",
    "        zip_ref.extractall(\"./\")\n",
    "    print(\"Unzipped data to {}\".format(Path(project_name)))\n",
    "    zip_path.unlink()\n",
    "\n",
    "\n",
    "extract_data(\n",
    "    zip_url=\"https://owncloud.mpi-cbg.de/index.php/s/xwYonC9LucjLsY6/download\",\n",
    "    project_name=\"monuseg-2018\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c25b6f7",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 2
   },
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "### Task 0.1\n",
    "Use the Explorer (left panel) and manually check the directory structure of the downloaded data. Where are the images and masks stored?  Can you programmatically count the number of images and masks?\n",
    "\n",
    "*Hint*: you can run any bash command in a jupyter notebook by prefixing it with `!`. You might find the command `wc` and the pipe operator `|` useful here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe75e22b",
   "metadata": {
    "lines_to_next_cell": 2,
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "##########################\n",
    "####### Solution #########\n",
    "##########################\n",
    "\n",
    "!ls -l monuseg-2018/download/images | wc -l\n",
    "!ls -l monuseg-2018/download/masks | wc -l"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdfcc59b",
   "metadata": {},
   "source": [
    "## Chapter 1: Understanding the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfd68096",
   "metadata": {},
   "source": [
    "### Image Basic Concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6951eac",
   "metadata": {},
   "source": [
    "In this chapter, we will explore the dataset and build some basic understanding of the data.\n",
    "\n",
    "2D Images are often represented as numpy arrays of shape (`height`, `width`, `num_channels`).\n",
    "Let's first load the images and masks and visualize them.\n",
    "\n",
    "![RGB image as a np array](https://github.com/dlmbl/boot/assets/34229641/ce1ad3f3-dc34-46d1-b301-198768fbc369)\n",
    "\n",
    "<div style=\"text-align: right\"> Credit: <a href=\"https://e2eml.school/convert_rgb_to_grayscale.html\">Brandon Rohrerâ€™s Blog</a></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fa8afaf",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 2
   },
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "### Task 1.1\n",
    "Load the images and masks using `tifffile.imread`. Define a `visualize` function using `matplotlib.pyplot.imshow` to display the image and mask side by side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c6a85a",
   "metadata": {
    "lines_to_next_cell": 2,
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "##########################\n",
    "######## Solution ###########\n",
    "##########################\n",
    "from tifffile import imread\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "img_path = \"monuseg-2018/download/images/TCGA-2Z-A9J9-01A-01-TS1.tif\"\n",
    "mask_path = \"monuseg-2018/download/masks/TCGA-2Z-A9J9-01A-01-TS1.tif\"\n",
    "\n",
    "img = imread(img_path)\n",
    "mask = imread(mask_path)\n",
    "\n",
    "\n",
    "def visualize(im1, im2):\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.subplot(121)\n",
    "    plt.imshow(im1)\n",
    "    plt.subplot(122)\n",
    "    plt.imshow(im2)\n",
    "    plt.tight_layout()\n",
    "\n",
    "\n",
    "visualize(img, mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "191f86c6",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "### Task 1.2\n",
    "How many channels does the image have? How about the mask?\n",
    "\n",
    "*Hint*: <a href=\"https://assets.datacamp.com/blog_assets/Numpy_Python_Cheat_Sheet.pdf\">np cheatsheet</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54a4c26f",
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "##########################\n",
    "######## Solution ###########\n",
    "##########################\n",
    "\n",
    "print(img.shape)\n",
    "print(mask.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c58b6c1",
   "metadata": {},
   "source": [
    "Next, let's figure out the data type of the images and masks.\n",
    "Images can be represented by a variety of data types. It is important to understand the data type of images and what values they can take.\n",
    "For example, here are some common data types used in images:\n",
    "- `bool`: binary, 0 or 1\n",
    "- `uint8`: unsigned integers, 0 to 255 range\n",
    "- `int8`: signed integers, -128 to 127 range\n",
    "- `float32`: floating point numbers, 32-bit precision\n",
    "\n",
    "Here is a comprehensive [guide](https://numpy.org/doc/stable/reference/arrays.scalars.html#numpy.number) on numpy data types."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "368b46fc",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "### Task 1.3\n",
    "What is the data type of <code>img</code> and the <code>mask</code> ? What are the minimum and maximum intensity values?\n",
    "\n",
    "*Hint*: <a href=\"https://assets.datacamp.com/blog_assets/Numpy_Python_Cheat_Sheet.pdf\">np cheatsheet</a></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ad23d8",
   "metadata": {
    "lines_to_next_cell": 2,
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "##########################\n",
    "####### Solution #########\n",
    "##########################\n",
    "\n",
    "print(\"data type: \", img.dtype, mask.dtype)\n",
    "print(\"Image min and max: \", img.min(), img.max())\n",
    "print(\"Mask min and max: \", mask.min(), mask.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "272310af",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "### Task 1.4\n",
    "Let's dive deeper into the mask. What does the minimum and maximum value of the mask represent?\n",
    "How many unique values does the mask have? Can you visualize the region where the values is 0 and where the value is 100?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80988f45",
   "metadata": {
    "lines_to_next_cell": 2,
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "##########################\n",
    "####### Solution #########\n",
    "##########################\n",
    "import numpy as np\n",
    "\n",
    "unique_values = np.unique(mask)\n",
    "print(f\"There are {len(unique_values)} unique values in mask.\")\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(121)\n",
    "plt.imshow(mask == 0)\n",
    "plt.subplot(122)\n",
    "plt.imshow(mask == 100)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3331170f",
   "metadata": {},
   "source": [
    "### Transpose images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb57c6d6",
   "metadata": {},
   "source": [
    "In deep learning, images are represented as (`num_channels`, `height`, `width`).\n",
    "But the image which we are working with has the `channel` as the last axis.\n",
    "Therefore, we need to reshape (by swapping) the image to the correct shape."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8fd211d",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "### Task 1.5\n",
    "Transpose the image to have the channel as the first axis. Use `np.transpose` to achieve this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "491a1869",
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "##########################\n",
    "####### Solution #########\n",
    "##########################\n",
    "reshaped_img = np.transpose(img, (2, 0, 1))\n",
    "print(\"Reshaped image shape: \", reshaped_img.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74810c03",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 2
   },
   "source": [
    "<div class=\"alert alert-success\">\n",
    "\n",
    "## Checkpoint 1\n",
    "\n",
    "Great Job! ðŸŽŠ Please flag the sticky note when you reach this checkpoint.\n",
    "\n",
    "In the first chapter, we learned about:\n",
    "\n",
    "<li> image data type and shape </li>\n",
    "<li> reshaping images </li>\n",
    "<li> visualizing images </li>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85bfe23b",
   "metadata": {},
   "source": [
    "## Chapter 2: Data Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b095ee88",
   "metadata": {},
   "source": [
    "Data augmentation is a technique used to artificially increase the size of a dataset by applying various transformations to the existing data. This is particularly useful in deep learning, where large datasets are often required to\n",
    "train models effectively. In this chapter, we will explore how to perform data augmentation using numpy and pytorch.\n",
    "\n",
    "Let's start with implementing some basic transformations, including flipping, rotation, cropping, and scaling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6c29ebe",
   "metadata": {},
   "source": [
    "### Implementing transformations with numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "326f103e",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "### Task 2.1\n",
    "Flip the image horizontally and vertically using numpy functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ecb131e",
   "metadata": {
    "lines_to_next_cell": 2,
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "###########################\n",
    "####### Solution #########\n",
    "###########################\n",
    "def flip_image(im):\n",
    "    flipped_horizontally = np.flip(im, axis=0)  # Flip horizontally\n",
    "    flipped_vertically = np.flip(im, axis=1)  # Flip vertically\n",
    "    return flipped_horizontally, flipped_vertically\n",
    "\n",
    "\n",
    "flipped_horizontally, flipped_vertically = flip_image(img)\n",
    "visualize(img, flipped_horizontally)\n",
    "visualize(img, flipped_vertically)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ea0f385",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "### Task 2.2\n",
    "Implement a function to flip the image horizontally and vertically by directly manipulating the arrays without any additional functions.\n",
    "\n",
    "Hint: you will find numpy [slicing and striding](https://numpy.org/doc/stable/user/basics.indexing.html#slicing-and-striding) useful here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d16dbda",
   "metadata": {
    "lines_to_next_cell": 2,
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "###########################\n",
    "####### Solution #########\n",
    "###########################\n",
    "def flip_image(im):\n",
    "    flipped_horizontally = im[::-1, :, :]  # Flip horizontally\n",
    "    flipped_vertically = im[:, ::-1, :]  # Flip vertically\n",
    "    return flipped_horizontally, flipped_vertically\n",
    "\n",
    "\n",
    "flipped_horizontally, flipped_vertically = flip_image(img)\n",
    "visualize(img, flipped_horizontally)\n",
    "visualize(img, flipped_vertically)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2afd269",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "### Task 2.3\n",
    "Rotate the image by 90 degrees clockwise and counter-clockwise using numpy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bf746bf",
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "##########################\n",
    "####### Solution #########\n",
    "##########################\n",
    "def rotate_image(image):\n",
    "    rotated_clockwise = np.rot90(image, k=-1)  # Rotate clockwise\n",
    "    rotated_counter_clockwise = np.rot90(image, k=1)  # Rotate counter-clockwise\n",
    "    return rotated_clockwise, rotated_counter_clockwise\n",
    "\n",
    "\n",
    "rotated_clockwise, rotated_counter_clockwise = rotate_image(img)\n",
    "visualize(img, rotated_clockwise)\n",
    "visualize(img, rotated_counter_clockwise)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbe7153f",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "### Task 2.4\n",
    "Implement a function to crop out the top left quardrant of the image and rescale it to the original size using numpy.\n",
    "\n",
    "Hint: `skimage.transform.resize` can be used to rescale the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78aebc0f",
   "metadata": {
    "lines_to_next_cell": 2,
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "##########################\n",
    "####### Solution #########\n",
    "##########################\n",
    "from skimage.transform import resize\n",
    "\n",
    "\n",
    "def crop_and_rescale(im):\n",
    "    height, width, _ = im.shape\n",
    "    top_left = im[: height // 2, : width // 2, :]\n",
    "\n",
    "    # Rescale to original size\n",
    "    top_left_rescaled = resize(top_left, im.shape)\n",
    "    return top_left_rescaled\n",
    "\n",
    "\n",
    "top_left_rescaled = crop_and_rescale(img)\n",
    "visualize(img, top_left_rescaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "163e94cc",
   "metadata": {},
   "source": [
    "### Implementing transformations with pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f75889a",
   "metadata": {},
   "source": [
    "Now that we have implemented basic transformations using numpy, let's explore how to do the same using PyTorch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcb1b449",
   "metadata": {},
   "source": [
    "PyTorch provides a powerful library called `torchvision` that includes many built-in transformations for\n",
    "data augmentation. These transformations can be easily applied to images and are optimized for performance.\n",
    "The `transforms` module provides a wide range of transformations that can be applied to images.\n",
    "We can compose multiple transformations together using `transforms.Compose` and randomly apply them to the images on-the-fly during training.\n",
    "Here is an example of how to use `torchvision.transforms` to perform some of transformations as above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f273f6ed",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "### Task 2.5\n",
    "Let's compose a series of transformations using `transforms.Compose()` that includes:\n",
    "- Randomly flip the image horizontally with a probability of 0.5\n",
    "- Randomly flip the image vertically with a probability of 0.5\n",
    "- Randomly rotate the image by 90 degrees\n",
    "- Randomly crop the image to a size of 500x500\n",
    "- Resize the image to a size of 1000x1000\n",
    "\n",
    "Hint: first, convert the numpy array to a PIL image using `transforms.ToPILImage()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7809b942",
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "##########################\n",
    "####### Solution #########\n",
    "##########################\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Define a series of transformations\n",
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToPILImage(),  # Convert numpy array to PIL Image\n",
    "        transforms.RandomHorizontalFlip(p=0.5),  # Randomly flip the image horizontally\n",
    "        transforms.RandomVerticalFlip(p=0.5),  # Randomly flip the image vertically\n",
    "        transforms.RandomRotation(degrees=90),  # Randomly rotate the image by 90\n",
    "        transforms.RandomCrop(size=(500, 500)),  # Randomly crop the image to 500x500\n",
    "        transforms.Resize(size=(1000, 1000)),  # Resize the image to 1000x1000\n",
    "    ]\n",
    ")\n",
    "\n",
    "transformed_img = transform(img)\n",
    "visualize(img, transformed_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6188bbba",
   "metadata": {},
   "source": [
    "### Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "167aafac",
   "metadata": {},
   "source": [
    "After applying the transformations, the images are often normalized before being fed into the model.\n",
    "Normalization is a technique used to scale the pixel values of an image to a specific range, typically [0, 1] or [-1, 1].\n",
    "This helps in stabilizing the training process and improving the convergence of the model.\n",
    "\n",
    "One way of normalizing an image is to divide the intensity on each pixel by the maximum allowed intensity for the available data type."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "191bbe9d",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "### Task 2.5\n",
    "Normalize the image by dividing each pixel value by the maximum allowed intensity for the data type. Does the data type of the image change? What are the minimum and maximum values of the normalized image?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a262bc",
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "##########################\n",
    "####### Solution #########\n",
    "##########################\n",
    "normalized_img = img / 255.0\n",
    "print(\"Normalized image: \", normalized_img.dtype)\n",
    "print(\"Normalized image min: \", normalized_img.min(), \"max: \", normalized_img.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4eb449d",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "<div class=\"alert alert-success\">\n",
    "\n",
    "## Checkpoint 2\n",
    "\n",
    "Wow! ðŸ¤Ÿ Flag the sticky note when you reach this checkpoint!\n",
    "\n",
    "In the second chapter, we learnt about:\n",
    "\n",
    "<li> data augmentation methods and implementation in numpy </li>\n",
    "<li> pytorch transformations </li>\n",
    "<li> how to normalize images </li>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27d93f35",
   "metadata": {},
   "source": [
    "## Chapter 3: Convolutions\n",
    "\n",
    "### Implementing convolutions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29b27557",
   "metadata": {},
   "source": [
    "Convolutions are the elementary operations used in Convolutional Neural Networks (CNNs). <br> The images are convolved with filters as below: <br>\n",
    "\n",
    "![](https://upload.wikimedia.org/wikipedia/commons/1/19/2D_Convolution_Animation.gif)\n",
    "\n",
    "\n",
    "Please read this section https://en.wikipedia.org/wiki/Kernel_(image_processing)#Convolution on convolutions to learn how to implement a your own convolution function!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "246ebdc7",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "### Task 3.1\n",
    "Implement a function that performs a convolution of an image with a filter. \n",
    "<br> Assume that your image is square and that your filter is square and has an odd width and height.\n",
    "<br> Also assume that stride is 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2992b959",
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "##########################\n",
    "####### Solution #########\n",
    "##########################\n",
    "\n",
    "\n",
    "def conv2d(img, kernel):\n",
    "    assert kernel.shape[0] == kernel.shape[1]\n",
    "    assert kernel.shape[0] % 2 != 0\n",
    "\n",
    "    h, w = img.shape[0], img.shape[1]  # Starting size of image\n",
    "    d_k = kernel.shape[0]  # Size of kernel\n",
    "\n",
    "    h_new = h - d_k + 1  # Calculate the new height of the array\n",
    "    w_new = w - d_k + 1  # Calculate the new width of the array\n",
    "    output = np.zeros((h_new, w_new))\n",
    "\n",
    "    for i in range(output.shape[0]):\n",
    "        for j in range(output.shape[1]):\n",
    "            output[i, j] = np.sum(img[i : i + d_k, j : j + d_k] * kernel)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d88bc7",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Run this cell to check your function\n",
    "\n",
    "identity = np.array([[0, 0, 0], [0, 1, 0], [0, 0, 0]])\n",
    "# Let's take a 256x256 center crop of the image for better visualization of the effect of the convolution\n",
    "new_im = conv2d(img[128:384, 128:384, 0], identity)\n",
    "# Lets print the original image and the convolved image\n",
    "print(img[128:384, 128:384, 0].shape)\n",
    "print(new_im.shape)\n",
    "\n",
    "# Lets visualize the original image and the convolved image and the filter\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.subplot(131)\n",
    "plt.imshow(img[128:384, 128:384, 0])\n",
    "plt.title(\"Original Image\")\n",
    "plt.subplot(132)\n",
    "plt.imshow(identity)\n",
    "plt.title(\"Kernel\")\n",
    "plt.subplot(133)\n",
    "plt.imshow(new_im)\n",
    "plt.title(\"Convolved Image\")\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "160e89e2",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 0
   },
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "### Task 3.2\n",
    "\n",
    "We noticed that the output image is smaller than the input image! <br>\n",
    "\n",
    "Given an input image of size $H \\times W$, a filter of size $K_h \\times K_w$ , and strides $S_h$ and $S_w$, \n",
    "can you come up with an analytical relationship regarding how much smaller the output image is compared to the input image?\n",
    "\n",
    "Feel free to play with this [visualizer](https://ezyang.github.io/convolution-visualizer/index.html) to get an intuition (ignore \"Padding\" and \"Dilation\" for now)!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58e4da9f",
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "source": [
    "\n",
    "```\n",
    "##########################\n",
    "####### Solution #########\n",
    "##########################\n",
    "```\n",
    "\n",
    "- Given an input image of size $H \\times W$, a filter of size $K_h \\times K_w$ , and strides $S_h$ and $S_w$\n",
    "the output size (height $H_{out}$ and width $W_{out}$) can be calculated using the following formulas (Note that $\\lfloor.\\rfloor$ is the floor operator):\n",
    "\n",
    "$$\n",
    "\\begin{equation*}\n",
    "    H_{out} = \\left\\lfloor \\frac{H - K_h}{S_h} \\right\\rfloor + 1\n",
    "\\end{equation*}\n",
    "$$\n",
    "$$\n",
    "\\begin{equation*}\n",
    "    W_{out} = \\left\\lfloor \\frac{W - K_w}{S_w} \\right\\rfloor + 1\n",
    "\\end{equation*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc21c113",
   "metadata": {},
   "source": [
    "### Different types of kernels\n",
    "\n",
    "Let's explore how different kernels affect the output image.\n",
    "The following is known as the [Sobel filter](https://en.wikipedia.org/wiki/Sobel_operator):\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "    1 & 2 & 1 \\\\\n",
    "    0 & 0 & 0 \\\\\n",
    "    -1 & -2 & -1\n",
    "\\end{bmatrix}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bf4a7c1",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "### Task 3.3\n",
    "\n",
    "Apply the Sobel filter and describe the output image. What features does it highlight? <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bb32579",
   "metadata": {
    "lines_to_next_cell": 2,
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "##########################\n",
    "####### Solution #########\n",
    "##########################\n",
    "\n",
    "filter = np.array([[1, 2, 1], [0, 0, 0], [-1, -2, -1]])\n",
    "output_img = conv2d(img[128:384, 128:384, 0], filter)\n",
    "visualize(img[128:384, 128:384, 0], output_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e5f2f81",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "### Task 3.4 (Bonus)\n",
    "\n",
    "Try different [kernels](https://www.geeksforgeeks.org/deep-learning/types-of-convolution-kernels/#basic-convolution-kernels) and visualize the results. <br>\n",
    "What features do they highlight? <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4af6fcff",
   "metadata": {
    "lines_to_next_cell": 2,
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "##########################\n",
    "####### Solution #########\n",
    "##########################\n",
    "\n",
    "# Up to you to try different filters!\n",
    "filter = np.array(\n",
    "    [[0, -1, 0], [-1, 4, -1], [0, -1, 0]]\n",
    ")  # For example, Laplacian filter\n",
    "output_img = conv2d(img[128:384, 128:384, 0], filter)\n",
    "visualize(img[128:384, 128:384, 0], output_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cb27082",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "Convolutions are a powerful tool for extracting features from images. They are the basic building blocks of Convolutional Neural Networks (CNNs), which are widely used in computer vision tasks.\n",
    "In these exercises, we defined the kernels ourself, but in practice, the kernels are learned during the training process of the CNN."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afcf610e",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 2
   },
   "source": [
    "<div class=\"alert alert-success\">\n",
    "\n",
    "Good job! ðŸ¤Ÿ Flag the sticky note when you reach this checkpoint!\n",
    "\n",
    "## Checkpoint 3\n",
    "\n",
    "In the third chapter, we learnt about:\n",
    "\n",
    "<li> Convolutions and its implementation </li>\n",
    "<li> Different types of kernels </li>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab8d9c62",
   "metadata": {},
   "source": [
    "## Chapter 4: Batching\n",
    "### Loading data and Sampling a batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6071f7c1",
   "metadata": {},
   "source": [
    "In this chapter, we will learn how to create batches of images and masks.\n",
    "Batching is a technique used to group multiple samples together to speed up the training process and make better use of the GPU memory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6afa7999",
   "metadata": {},
   "source": [
    "We will use the `glob` module to load all the images and masks from the `monuseg-2018/download/images` and `monuseg-2018/download/masks` directories.\n",
    "`glob` is a module that allows us to search for files and directories matching a specified pattern."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "036c9681",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "### Task 4.1\n",
    "Load all the images and masks using `glob` and `tifffile.imread`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6537d481",
   "metadata": {
    "lines_to_next_cell": 0,
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "##########################\n",
    "####### Solution #########\n",
    "##########################\n",
    "from glob import glob\n",
    "import tifffile as tiff\n",
    "\n",
    "image_paths = glob(\"monuseg-2018/download/images/*.tif\")\n",
    "mask_paths = glob(\"monuseg-2018/download/masks/*.tif\")\n",
    "images = [tiff.imread(path) for path in image_paths]\n",
    "masks = [tiff.imread(path) for path in mask_paths]\n",
    "print(f\"Loaded {len(images)} images and {len(masks)} masks.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbf38716",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "Now that we have loaded the images and masks, we can create a mini-batch of images and masks.\n",
    "A mini-batch is a small subset of the dataset that is used to train the model in one iteration."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccc068f0",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "### Task 4.2\n",
    "Sample 5 images and masks from the loaded data and create a mini-batch of images and masks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dab607b",
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "##########################\n",
    "####### Solution #########\n",
    "##########################\n",
    "import random\n",
    "\n",
    "batch_size = 5\n",
    "indices = random.sample(range(len(images)), batch_size)\n",
    "batch_images = [images[i] for i in indices]\n",
    "batch_masks = [masks[i] for i in indices]\n",
    "print(\n",
    "    f\"Created a mini-batch of {len(batch_images)} images and {len(batch_masks)} masks.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc104563",
   "metadata": {},
   "source": [
    "### Pytorch dataset and dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36fac00f",
   "metadata": {},
   "source": [
    "We can also create a custom dataset using PyTorch's `Dataset` class.\n",
    "A custom dataset can be created by subclassing the `torch.utils.data.Dataset` class and implementing the `__init__`, `__len__` and `__getitem__` methods.\n",
    "The `__init__` method is an initialization procedure. The `__len__` method returns the number of samples in the dataset, and the `__getitem__` method returns a sample from the dataset at a given index."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa5ec0f2",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "### Task 4.3\n",
    "Create a custom dataset using PyTorch's `Dataset` class that loads the images and masks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e2f1223",
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "##########################\n",
    "####### Solution #########\n",
    "##########################\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, images, masks):\n",
    "        self.images = images\n",
    "        self.masks = masks\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.images[idx]\n",
    "        mask = self.masks[idx]\n",
    "        return image, mask\n",
    "\n",
    "\n",
    "my_dataset = MyDataset(images, masks)\n",
    "print(len(my_dataset))\n",
    "print(my_dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af42fbb5",
   "metadata": {},
   "source": [
    "Now that we have created a custom dataset, we can use it to create batches of images and masks.\n",
    "We can use the `DataLoader` class from PyTorch to create batches of images and masks.\n",
    "The `DataLoader` class takes a dataset as input and provides an iterable over the dataset, allowing us to easily load the images and masks in batches during training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af73d962",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "### Task 4.4\n",
    "Create a `DataLoader` to load the dataset in batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e50a1f5f",
   "metadata": {
    "lines_to_next_cell": 0,
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "##########################\n",
    "####### Solution #########\n",
    "##########################\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "batch_size = 5\n",
    "data_loader = DataLoader(my_dataset, batch_size=batch_size)\n",
    "for batch_images, batch_masks in data_loader:\n",
    "    print(f\"Loaded a batch of {len(batch_images)} images and {len(batch_masks)} masks.\")\n",
    "    break  # Just to check the first batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11545baf",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 2
   },
   "source": [
    "<div class=\"alert alert-success\">\n",
    "\n",
    "Nice! ðŸ¤Ÿ Flag the sticky note when you reach this checkpoint!\n",
    "\n",
    "## Checkpoint 4\n",
    "\n",
    "In the fourth chapter, we learnt about:\n",
    "\n",
    "<li> Using `glob` to find all data that matched certain pattern </li>\n",
    "<li> Batching data </li>\n",
    "<li> Using pytorch dataset and dataloader </li>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc9e10a5",
   "metadata": {},
   "source": [
    "## Chapter 5: Advanced Analysis (Bonus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5342670f",
   "metadata": {},
   "source": [
    "This chapter focuses on a more advanced analysis of the image data and masks.\n",
    "We will begin by analyzing cell sizes to visualize their distribution and then create an overlay of the masks on the original images.\n",
    "These analyses are crucial for gaining a better understanding of the dataset and closely examining the quality of our segmentation results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "516d1ae8",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "### Task 5.1 (Bonus)\n",
    "Let's find the sizes of the cells in the image and visualize the distribution.\n",
    "\n",
    "Hint: `skimage.measure.regionprops` can be useful here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "679d989b",
   "metadata": {
    "lines_to_next_cell": 2,
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "##########################\n",
    "####### Solution #########\n",
    "##########################\n",
    "from skimage import measure\n",
    "\n",
    "\n",
    "def analyze_area(mask):\n",
    "    regions = measure.regionprops(mask)\n",
    "    areas = [region.area for region in regions]\n",
    "    plt.hist(areas, bins=50)\n",
    "    plt.xlabel(\"Size\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.title(\"Histogram of Cell Sizes\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "analyze_area(mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88f58f2d",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "### Task 5.2 (Bonus)\n",
    "\n",
    "Let's overlay the masks' boundaries on the images to visualize the results.\n",
    "\n",
    "Hint: `skimage.segmentation.find_boundaries` can be useful here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83898279",
   "metadata": {
    "lines_to_next_cell": 2,
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "##########################\n",
    "####### Solution #########\n",
    "##########################\n",
    "from skimage.segmentation import mark_boundaries\n",
    "\n",
    "\n",
    "def overlay_masks_on_images(im, mask):\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    combined_im = mark_boundaries(im, mask)\n",
    "    plt.imshow(combined_im)\n",
    "\n",
    "\n",
    "overlay_masks_on_images(img[:250, :250], mask[:250, :250])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6818f341",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "<div class=\"alert alert-success\">\n",
    "\n",
    "Hurrah! ðŸ˜ƒ Post in the chat when you reach this checkpoint! \n",
    "\n",
    "## Checkpoint 5 (Bonus)\n",
    "\n",
    "In this chapter, we learned about:\n",
    "\n",
    "<li> analyzing the size of cells in the images </li>\n",
    "<li> visualizing the masks on top of the images </li>"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "all",
   "main_language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
